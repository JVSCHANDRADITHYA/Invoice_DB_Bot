"""
streamlit_invoice_app.py

Single-file Streamlit application that:
 - Accepts a CSV upload (expected schema columns as provided)
 - Persists a DuckDB database under 'Databases File/data.duckdb'
 - Builds a local Chroma vector DB under 'Databases File/chroma' for quick resource lookups
 - Provides a button to erase all stored databases (resets to UI initial state)
 - Shows verbose operation logs for each step
 - Chat interface: NL query -> Ollama (gpt-oss) -> SQL -> DuckDB execution -> results -> Ollama explanation
 - Invoice generation: user requests invoice for a Resource ID or Resource Name; human-in-loop clarifies financial period etc; app queries DB for hours & rate, fills a Jinja2 HTML invoice template and offers it for download. Also stores generated invoices in 'Databases File/invoices'

Notes:
 - Configure the OLLAMA_URL via environment variable OLLAMA_URL (defaults to http://localhost:11434/api/chat)
 - This is a demo-level app: in production add stronger sanitization, authentication, and robust SQL parsing.

Requirements (pip):
pip install streamlit pandas duckdb chromadb sentence-transformers requests jinja2 python-multipart

Run:
streamlit run streamlit_invoice_app.py

"""

import os
import io
import shutil
import json
import uuid
import datetime
from typing import Optional, Dict, Any, List

import streamlit as st
import pandas as pd
import duckdb
import requests
from jinja2 import Template

# chromadb imports
import chromadb
from chromadb import Settings

from sentence_transformers import SentenceTransformer

# ------------------ Configuration ------------------
BASE_FOLDER = os.path.abspath("Databases File")
DUCKDB_PATH = os.path.join(BASE_FOLDER, "data.duckdb")
CHROMA_PERSIST_DIR = os.path.join(BASE_FOLDER, "chroma")
INVOICE_FOLDER = os.path.join(BASE_FOLDER, "invoices")
NAMES_COLLECTION = "resources"

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434/api/chat")
MODEL_NAME = os.environ.get("OLLAMA_MODEL", "gpt-oss")
EMBED_MODEL_NAME = os.environ.get("EMBED_MODEL", "all-MiniLM-L6-v2")

# Expected schema columns (strict)
SCHEMA_COLUMNS = [
    "Project Financial Location","Project ID","Project Name","Project Manager",
    "Resource Name","Resource ID","Resource Financial Location","Posted Hours",
    "Project Task Name","Project Task ID","Actual Date","Posted Date",
    "Financial Period (Posted Date)","Resource Financial Department",
    "Project Financial Department","Project Class","Timesheet Week (Actual Date)",
    "Timesheet Week (Posted Date)","Resource Rate","Project Rate",
    "Resource Primary Role","Resource Project Role","Resource Currency"
]
TABLE_NAME = "sample_table"

SYSTEM_PROMPT = """
You are an SQL query generator.
Your ONLY job is to output a valid SQL query that uses EXACTLY the column names shown in the schema and the table name sample_table.
Always wrap column names in double quotes.
Output ONLY the SQL query, no explanation.
"""

# Simple Jinja2 HTML invoice template (based on user's provided template)
INVOICE_TEMPLATE = """
<!doctype html>
<html>
<head>
  <meta charset="utf-8">
  <title>Invoice {{ invoice_no }}</title>
  <style>
    body { font-family: Arial, sans-serif; font-size: 14px }
    table { width: 100%; border-collapse: collapse }
    th, td { border: 1px solid #333; padding: 6px }
    .header { margin-bottom: 12px }
  </style>
</head>
<body>
  <div class="header">
    <h2>Tax Invoice - {{ invoice_no }}</h2>
    <div><strong>Invoice Date:</strong> {{ invoice_date }}</div>
  </div>

  <div>
    <strong>Project Name:</strong> {{ Project_Name }}<br>
    <strong>Resource Name:</strong> {{ resource_Name }}<br>
    <strong>Resource ID:</strong> {{ resource_id }}<br>
    <strong>Financial Period:</strong> {{ financial_period }}
  </div>

  <h3>Line Items</h3>
  <table>
    <thead><tr><th>S No</th><th>Description</th><th>Hours</th><th>Rate</th><th>Amount</th></tr></thead>
    <tbody>
      <tr>
        <td>1</td>
        <td>{{ Project_Name }}<br>{{ resource_Name }}</td>
        <td>{{ no_of_hours }}</td>
        <td>{{ resource_rate }}</td>
        <td>{{ total_amount }}</td>
      </tr>
    </tbody>
  </table>

  <p><strong>Total: </strong> {{ total_amount }}</p>

  <hr>
  <small>Generated by streamlit_invoice_app at {{ generated_at }}</small>
</body>
</html>
"""

# ------------------ Utilities ------------------

def ensure_folders():
    os.makedirs(BASE_FOLDER, exist_ok=True)
    os.makedirs(INVOICE_FOLDER, exist_ok=True)


def reset_all_data():
    try:
        duckdb.connect(DUCKDB_PATH).close()
    except:
        pass

    if os.path.exists(BASE_FOLDER):
        shutil.rmtree(BASE_FOLDER)

    ensure_folders()


def save_csv_to_duckdb(df: pd.DataFrame) -> None:
    ensure_folders()

    # ✅ FORCE correct numeric types at pandas level first
    df["Posted Hours"] = pd.to_numeric(df["Posted Hours"], errors="coerce")
    df["Resource Rate"] = pd.to_numeric(df["Resource Rate"], errors="coerce")

    con = duckdb.connect(DUCKDB_PATH)

    try:
        con.execute(f"DROP TABLE IF EXISTS {TABLE_NAME}")

        con.register("df_view", df)

        # ✅ FORCE correct DuckDB schema via CAST
        con.execute(f"""
            CREATE TABLE {TABLE_NAME} AS
            SELECT
                "Project Financial Location",
                "Project ID",
                "Project Name",
                "Project Manager",
                "Resource Name",
                "Resource Financial Location",
                CAST("Posted Hours" AS DOUBLE) AS "Posted Hours",
                "Project Task Name",
                "Project Task ID",
                "Actual Date",
                "Posted Date",
                "Financial Period (Posted Date)",
                "Resource ID",
                "Resource Financial Department",
                "Project Financial Department",
                "Project Class",
                "Timesheet Week (Actual Date)",
                "Timesheet Week (Posted Date)",
                CAST("Resource Rate" AS DOUBLE) AS "Resource Rate",
                "Project Rate",
                "Resource Primary Role",
                "Resource Project Role",
                "Resource Currency"
            FROM df_view
        """)

        con.unregister("df_view")

    finally:
        con.close()



# ----------- Chroma setup & embeddings -----------

class ChromaWrapper:
    def __init__(self, persist_directory: str = CHROMA_PERSIST_DIR):
        self.client = chromadb.Client(
            Settings(persist_directory=persist_directory)
        )

        self.embedder = SentenceTransformer(EMBED_MODEL_NAME)

        try:
            self.col = self.client.get_collection(name=NAMES_COLLECTION)
        except Exception:
            self.col = self.client.create_collection(name=NAMES_COLLECTION)

    def _embed(self, texts: List[str]):
        vecs = self.embedder.encode(texts, convert_to_numpy=True).tolist()
        return vecs

    def ingest_resources_from_df(self, df: pd.DataFrame):
        docs = []
        metadatas = []
        ids = []

        seen = set()  

        for idx, row in df.iterrows():
            rid_raw = str(row.get("Resource ID", "")).strip()

            # ✅ Make ID globally unique per row
            unique_id = f"{rid_raw}__{idx}"

            if unique_id in seen:
                continue
            seen.add(unique_id)

            name = str(row.get("Resource Name", "")).strip()
            project = str(row.get("Project Name", "")).strip()

            doc_text = (
                f"Resource:{name} | "
                f"ResourceID:{rid_raw} | "
                f"Project:{project} | "
                f"Manager:{row.get('Project Manager','')}"
            )

            docs.append(doc_text)

            metadatas.append({
                "Resource ID": rid_raw,   # ✅ ORIGINAL ID preserved in metadata
                "Resource Name": name,
                "Project Name": project,
                "Project ID": str(row.get("Project ID", ""))
            })

            ids.append(unique_id)  # ✅ UNIQUE VECTOR ID

        if not docs:
            return

        embeddings = self._embed(docs)

        self.col.upsert(
            documents=docs,
            metadatas=metadatas,
            ids=ids,
            embeddings=embeddings
        )


    def query(self, text: str, n_results: int = 5):
        results = self.col.query(
            query_texts=[text],
            n_results=n_results
        )

        hits = []
        for i, _id in enumerate(results.get("ids", [[]])[0]):
            meta = results.get("metadatas", [[]])[0][i]
            dist = results.get("distances", [[]])[0][i] if "distances" in results else None
            hits.append({"id": _id, "meta": meta, "distance": dist})

        return hits



# ------------------ Ollama interaction ------------------

def call_ollama_system(system_prompt: str, user_prompt: str, model_name: str = MODEL_NAME) -> str:
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "stream": False
    }
    resp = requests.post(OLLAMA_URL, json=payload, timeout=60)
    resp.raise_for_status()
    data = resp.json()
    # defensive extraction
    content = ""
    if isinstance(data, dict):
        if 'message' in data and isinstance(data['message'], dict):
            content = data['message'].get('content','')
        elif 'choices' in data and len(data['choices'])>0:
            content = data['choices'][0].get('message',{}).get('content','')
        else:
            content = json.dumps(data)
    return content.strip()


# ------------------ SQL validation & execution ------------------
import re
COLUMN_QUOTED_PATTERN = re.compile(r'"([^"]+)"')


def validate_sql_against_schema(sql: str) -> (bool, Optional[str]):
    if '```' in sql or '`' in sql:
        return False, 'Model output contains code fences/backticks.'
    if re.search(r'--|/\*|\*/', sql):
        return False, 'SQL contains comments.'
    # must reference table name
    if TABLE_NAME not in sql:
        return False, f"SQL must reference the table name exactly: {TABLE_NAME}"
    quoted = COLUMN_QUOTED_PATTERN.findall(sql)
    for q in quoted:
        if q not in SCHEMA_COLUMNS:
            return False, f'Unknown column used: "{q}"'
    if not re.match(r'^\s*(SELECT|WITH|UPDATE|DELETE|INSERT)\b', sql.strip(), re.IGNORECASE):
        return False, 'Output does not start with a SQL statement.'
    return True, None


def execute_sql_and_get_df(sql: str) -> pd.DataFrame:
    con = duckdb.connect(DUCKDB_PATH)
    try:
        df = con.execute(sql).fetchdf()
    finally:
        con.close()
    return df


# ------------------ Invoice generation helpers ------------------

def auto_invoice_no() -> str:
    # Example rule: INV-YYYYMMDD-XXXX
    ts = datetime.datetime.utcnow().strftime('%Y%m%d')
    rand = str(uuid.uuid4())[:4].upper()
    return f"INV-{ts}-{rand}"


def generate_invoice_html(context: Dict[str, Any]) -> str:
    tpl = Template(INVOICE_TEMPLATE)
    return tpl.render(**context)


# ------------------ Streamlit UI ------------------

st.set_page_config(page_title="Invoice Generator + NL→SQL", layout="wide")
st.title("Invoice Generator + NL→SQL (Ollama + DuckDB + Chroma)")

ensure_folders()
chroma_wrapper: Optional[ChromaWrapper] = None
if os.path.exists(CHROMA_PERSIST_DIR):
    try:
        chroma_wrapper = ChromaWrapper()
    except Exception:
        chroma_wrapper = None

# Sidebar controls
with st.sidebar:
    st.header("Data Controls")
    uploaded = st.file_uploader("Upload CSV (will create DuckDB + vector DB)", type=['csv'])
    if st.button("Erase all stored data (RESET)"):
        reset_all_data()
        st.success("All data erased. Restart the app if needed.")

    st.markdown("---")
    st.markdown("**Verbose Logs**")
    verbose = st.checkbox("Show verbose logs", value=True)

# Area to show logs
log_container = st.empty()

logs: List[str] = []

def log(msg: str):
    logs.append(f"[{datetime.datetime.now().isoformat()}] {msg}")
    if verbose:
        log_container.text('\n'.join(logs[-50:]))

# If CSV uploaded - ingest
if uploaded is not None:
    try:
        df = pd.read_csv(uploaded)
    except Exception as e:
        st.error(f"Failed to read CSV: {e}")
        st.stop()
    st.success(f"CSV read: {df.shape[0]} rows, {df.shape[1]} columns")
    st.write("Columns detected:", list(df.columns))
    # save CSV to disk
    csv_save_path = os.path.join(BASE_FOLDER, f"uploaded_{datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')}.csv")
    ensure_folders()
    df.to_csv(csv_save_path, index=False)
    log(f"Saved uploaded CSV to {csv_save_path}")
    # Save to duckdb
    try:
        save_csv_to_duckdb(df)
        log(f"Wrote data to DuckDB at {DUCKDB_PATH}")
    except Exception as e:
        st.error(f"Failed to write DuckDB: {e}")
        st.stop()
    # Build chroma index
    try:
        cw = ChromaWrapper()
        cw.ingest_resources_from_df(df)
        chroma_wrapper = cw
        log(f"Built/updated Chroma DB at {CHROMA_PERSIST_DIR} with {df.shape[0]} resources")
    except Exception as e:
        st.error(f"Failed to build Chroma index: {e}")
        chroma_wrapper = None

# Main columns
col1, col2 = st.columns([1,2])

with col1:
    st.subheader("Chat → NL to SQL")
    nl_query = st.text_area("Enter natural language query for data (will be converted to SQL)", height=120)
    if st.button("Run NL Query"):
        if not nl_query.strip():
            st.warning("Enter a query first.")
        else:
            # Send NL to Ollama to get SQL
            log("Sending NL to Ollama for SQL generation...")
            # Provide system prompt + schema
            schema_text = "Columns:\n" + "\n".join([f'"{c}"' for c in SCHEMA_COLUMNS])
            user_prompt = nl_query + "\n\n" + "Schema:\n" + schema_text + "\n\nRemember to only output a single SQL statement using the table name sample_table and double-quoted columns."
            try:
                model_out = call_ollama_system(SYSTEM_PROMPT, user_prompt)
                log(f"Model output (raw): {model_out}")
            except Exception as e:
                st.error(f"Ollama call failed: {e}")
                st.stop()
            # Validate
            ok, err = validate_sql_against_schema(model_out)
            if not ok:
                st.error(f"SQL validation failed: {err}")
                st.text_area("Raw model output", value=model_out, height=200)
            else:
                st.success("SQL validated. Executing on DuckDB...")
                try:
                    df_res = execute_sql_and_get_df(model_out)
                    st.dataframe(df_res)
                    log(f"Query executed: returned {len(df_res)} rows")
                except Exception as e:
                    st.error(f"Failed to execute SQL: {e}")
                    st.text_area("SQL", value=model_out, height=120)
                    st.stop()
                # send results back to model for interpretation
                try:
                    summary_prompt = f"I executed the following SQL:\n{model_out}\n\nResults (first 20 rows):\n{df_res.head(20).to_csv(index=False)}\n\nPlease provide a concise, human-friendly interpretation of these results (no SQL)."
                    interp = call_ollama_system("You are a helpful data interpreter.", summary_prompt)
                    st.markdown("**Model interpretation:**")
                    st.write(interp)
                except Exception as e:
                    st.warning(f"Failed to get interpretation from model: {e}")

with col2:
    st.subheader("Invoice generation")
    st.markdown("Generate invoice for a resource by Resource ID or Resource Name. The system will try to fetch metadata from DB and will ask clarifying questions (human-in-loop).")
    resource_query = st.text_input("Enter Resource ID or Resource Name to find")
    if st.button("Find resource"):
        if chroma_wrapper is None:
            st.warning("No Chroma index available. Upload CSV first.")
        elif not resource_query.strip():
            st.warning("Enter a Resource ID or name to search")
        else:
            hits = chroma_wrapper.query(resource_query, n_results=5)
            if not hits:
                st.info("No matches found in vector DB.")
            else:
                st.write("Top matches:")
                for i, h in enumerate(hits):
                    st.write(i+1, h['meta'], f"score:{h.get('distance')}")
                sel = st.number_input("Select rank to use (1..5)", min_value=1, max_value=len(hits), value=1)
                chosen = hits[sel-1]
                st.markdown("**Chosen resource metadata:**")
                st.json(chosen['meta'])
                # Offer to populate invoice fields from DB
                if st.button("Prepare invoice draft for chosen resource"):
                    # ask for financial period / year
                    st.info("Confirm financial period for invoice (e.g. 01-04-2025 to 31-03-2026)")
                    fin_from = st.date_input("Financial period start", value=datetime.date(datetime.date.today().year,4,1))
                    fin_to = st.date_input("Financial period end", value=datetime.date(datetime.date.today().year+1,3,31))
                    # additional fields
                    invoice_no = st.text_input("Invoice No (auto)", value=auto_invoice_no())
                    invoice_date = st.date_input("Invoice Date", value=datetime.date.today())
                    # Now query duckdb to sum hours for this resource & period
                    rid = chosen['meta'].get('Resource ID')
                    if not rid:
                        st.error("Chosen metadata lacks Resource ID; cannot auto-query hours.")
                    else:
                        # build SQL to sum Posted Hours for financial period
                        # Note: user stated that financial period is stored in "Financial Period (Posted Date)" column — we'll filter by that if available else by Posted Date range
                        con = duckdb.connect(DUCKDB_PATH)
                        try:
                            # Try by Financial Period column exact match first
                            fin_period_str = f"{fin_from.isoformat()} to {fin_to.isoformat()}"
                            q1 = f'SELECT SUM("Posted Hours") as total_hours, AVG(CAST("Resource Rate" AS DOUBLE)) as avg_rate FROM {TABLE_NAME} WHERE "Resource ID" = "{rid}" AND "Financial Period (Posted Date)" = \'{fin_period_str}\''
                            # But more robust: try Posted Date between
                            q2 = f"SELECT SUM(\"Posted Hours\") as total_hours, AVG(CAST(\"Resource Rate\" AS DOUBLE)) as avg_rate FROM {TABLE_NAME} WHERE \"Resource ID\" = '{rid}' AND \"Posted Date\" >= '{fin_from.isoformat()}' AND \"Posted Date\" <= '{fin_to.isoformat()}'"
                            try:
                                df_sum = con.execute(q2).fetchdf()
                            except Exception:
                                df_sum = con.execute(q1).fetchdf()
                        finally:
                            con.close()
                        total_hours = float(df_sum.at[0,'total_hours']) if 'total_hours' in df_sum.columns and not pd.isna(df_sum.at[0,'total_hours']) else 0.0
                        avg_rate = float(df_sum.at[0,'avg_rate']) if 'avg_rate' in df_sum.columns and not pd.isna(df_sum.at[0,'avg_rate']) else None
                        st.write(f"Total hours for selected period: {total_hours}")
                        st.write(f"Avg resource rate (from data): {avg_rate}")
                        # Ask human to confirm or edit rate
                        resource_rate = st.number_input("Resource rate to use (per hour)", value=float(avg_rate) if avg_rate else 0.0)
                        total_amount = total_hours * resource_rate
                        st.write(f"Calculated total amount: {total_amount}")

                        # Gather other invoice fields with defaults from metadata
                        context = {
                            'invoice_no': invoice_no,
                            'invoice_date': invoice_date.isoformat(),
                            'Del_note': '', 'mode_terms': '', 'ref_no': '', 'ref_date': '', 'others_ref': '',
                            'consignee_ship_to': chosen['meta'].get('Project Name',''), 'buy_ord_no':'', 'dated_2':'',
                            'dispatch_doc_no':'', 'del_note_date':'', 'dispatch_thr':'', 'destination':'',
                            'buyer_bill_to': chosen['meta'].get('Project Name',''), 'country':'India',
                            'terms_of_delivery':'',
                            'Project_Name': chosen['meta'].get('Project Name',''),
                            'resource_Name': chosen['meta'].get('Resource Name',''),
                            'resource_id': chosen['meta'].get('Resource ID',''),
                            'no_of_hours': total_hours,
                            'resource_rate': resource_rate,
                            'total_amount': total_amount,
                            'HSN_SAC':'', 'Quantity':'', 'rate':'', 'Amount': total_amount,
                            'generated_at': datetime.datetime.utcnow().isoformat(),
                            'financial_period': f"{fin_from.isoformat()} to {fin_to.isoformat()}"
                        }

                        if st.button("Generate Invoice (draft) and Save"):
                            html = generate_invoice_html(context)
                            fname = f"invoice_{context['resource_id']}_{context['invoice_no']}.html"
                            path = os.path.join(INVOICE_FOLDER, fname)
                            with open(path, 'w', encoding='utf-8') as f:
                                f.write(html)
                            st.success(f"Invoice generated and saved to {path}")
                            st.markdown("Download invoice (HTML)")
                            st.markdown(f"[Download invoice]({path})")

# Footer: show existing invoices
st.markdown("---")
st.subheader("Stored invoices")
if os.path.exists(INVOICE_FOLDER):
    invs = sorted(os.listdir(INVOICE_FOLDER))
    for inv in invs[-50:][::-1]:
        st.write(inv)


# End of app

